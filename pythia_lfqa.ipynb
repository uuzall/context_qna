{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, T5Tokenizer, T5ForConditionalGeneration, AutoModelForSeq2SeqLM, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "import torch.nn as nn \n",
    "from torch.nn import functional as F \n",
    "import torch \n",
    "import transformers.optimization as optim \n",
    "import torch.optim as torch_optim \n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import trange, tqdm\n",
    "import matplotlib.pyplot as plt \n",
    "from datasets import load_dataset \n",
    "from accelerate import Accelerator, DeepSpeedPlugin, accelerator\n",
    "import pickle as pkl \n",
    "import pandas as pd\n",
    "import transformers\n",
    "import wandb \n",
    "import pandas as pd \n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "if device == 'cuda:0': \n",
    "  print(torch.cuda.get_device_name()) \n",
    "else:\n",
    "  print(device) \n",
    "\n",
    "MAX_LEN = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('/media/uzal/New Volume/hf_models/pythia-1b-deduped-v0')\n",
    "tokenizer.pad_token = tokenizer.decode(1)\n",
    "tokenizer.padding_side = 'left'\n",
    "model = AutoModelForCausalLM.from_pretrained('/media/uzal/New Volume/hf_models/pythia-1b-deduped-v0', device_map=device, use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data): \n",
    "  all_train = list() \n",
    "  for i in (loop := tqdm(data.iloc, total=len(data))): \n",
    "    sen = '' \n",
    "    attn_mask = list() \n",
    "    passages = [x['text'] for x in i.output[-1]['provenance']]\n",
    "    sen += 'Context:\\n* ' + '* '.join(passages)\n",
    "    sen += '\\nQuestion: ' + i.input + '\\nAnswer: '\n",
    "    attn_mask += list(0 for _ in range(len(tokenizer(sen).input_ids)))\n",
    "    sen += i.output[0]['answer'] + tokenizer.eos_token\n",
    "    attn_mask += list(1 for _ in range(len(tokenizer(i.output[0]['answer'] + tokenizer.eos_token).input_ids)))\n",
    "    if len(attn_mask) < MAX_LEN: \n",
    "      all_train.append((sen, attn_mask))\n",
    "      loop.set_postfix(length=len(all_train))\n",
    "  return all_train\n",
    "\n",
    "# data = pd.read_json('/media/uzal/New Volume/data/lfqa/train_supporting_docs.json', lines=True)\n",
    "# test_data = pd.read_json('/media/uzal/New Volume/data/lfqa/validation_supporting_docs.json', lines=True)\n",
    "# all_train = process_data(data) \n",
    "# all_test = process_data(test_data) \n",
    "\n",
    "# with open('data/lfqa_train.pkl', 'wb') as file: \n",
    "#   pkl.dump(all_train, file)\n",
    "\n",
    "# with open('data/lfqa_test.pkl', 'wb') as file: \n",
    "#   pkl.dump(all_test, file)\n",
    "\n",
    "with open('data/lfqa_train.pkl', 'rb') as file: \n",
    "  all_train = pkl.load(file) \n",
    "\n",
    "with open('data/lfqa_test.pkl', 'rb') as file: \n",
    "  all_test = pkl.load(file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([138851, 1024]) 138851\n",
      "torch.Size([1868, 1024]) 1868\n"
     ]
    }
   ],
   "source": [
    "def get_attn_inputs(all_data): \n",
    "  all_attn = torch.zeros((len(all_data), MAX_LEN))\n",
    "  all_inputs = list()\n",
    "\n",
    "  for idx, (i, attn) in enumerate(all_data):\n",
    "    all_attn[idx, -len(attn):] = torch.tensor(attn) \n",
    "    all_inputs.append(i) \n",
    "  print(all_attn.shape, len(all_inputs))\n",
    "  return all_attn, all_inputs\n",
    "\n",
    "train_attn, train_inputs = get_attn_inputs(all_train)\n",
    "x_train = list(zip(train_inputs, train_attn))\n",
    "test_attn, test_inputs = get_attn_inputs(all_test)\n",
    "x_test = list(zip(test_inputs, test_attn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "def calc_loss(input_ids, logits, attn): \n",
    "\tshift_labels = input_ids[..., 1:].contiguous() \n",
    "\tshift_logits = logits[..., :-1, :].contiguous() \n",
    "\tshift_attn = attn[:, -input_ids.size(1):]\n",
    "\tshift_attn = shift_attn[..., :-1].contiguous()\n",
    "\tloss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\tloss *= shift_attn.view(-1)\n",
    "\treturn loss.sum() / shift_attn.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33muuzall\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e929047e0f074a5dbed3801b38e8ba77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668530899914914, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/uzal/New Volume/Programming/quac/wandb/run-20230726_000357-yumj6h1g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/uuzall/Question%20Answering%20with%20Context%20Bot/runs/yumj6h1g' target=\"_blank\">serene-water-9</a></strong> to <a href='https://wandb.ai/uuzall/Question%20Answering%20with%20Context%20Bot' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/uuzall/Question%20Answering%20with%20Context%20Bot' target=\"_blank\">https://wandb.ai/uuzall/Question%20Answering%20with%20Context%20Bot</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/uuzall/Question%20Answering%20with%20Context%20Bot/runs/yumj6h1g' target=\"_blank\">https://wandb.ai/uuzall/Question%20Answering%20with%20Context%20Bot/runs/yumj6h1g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "project_name = ''\n",
    "\n",
    "wandb.init(\n",
    "    project='Question Answering with Context Bot', \n",
    "    entity='uuzall', \n",
    "    sync_tensorboard=True, \n",
    "    name=project_name, \n",
    "    monitor_gym=True, \n",
    "    save_code=True,\n",
    ")\n",
    "\n",
    "writer = torch.utils.tensorboard.SummaryWriter(f'runs/{project_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 2e-5\n",
    "bs, scale_bs = 64, 2\n",
    "steps = bs // scale_bs \n",
    "test_loss, best_test_loss = 0, 100\n",
    "n_epochs = 2\n",
    "global_step = 0 \n",
    "train_dl = DataLoader(x_train, batch_size=scale_bs, shuffle=True, pin_memory=True)\n",
    "test_dl = DataLoader(x_test, batch_size=scale_bs, shuffle=False, pin_memory=True)\n",
    "\n",
    "optimizer = optim.Adafactor(model.parameters(), scale_parameter=False, relative_step=False, lr=lr)\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.03*n_epochs*len(train_dl)//steps), num_training_steps=(n_epochs*len(train_dl))//steps)\n",
    "\n",
    "accelerator = Accelerator(gradient_accumulation_steps=steps, mixed_precision='bf16') \n",
    "model, optimizer, train_dl, test_dl, scheduler = accelerator.prepare(model, optimizer, train_dl, test_dl, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_it(file_name, best_test_loss): \n",
    "  model.eval() \n",
    "  test_loss = 0\n",
    "  with torch.no_grad(): \n",
    "    for (x, attn) in test_dl: \n",
    "      inputs = tokenizer(x, return_tensors=\"pt\", max_length=MAX_LEN, padding='longest', truncation=True)\n",
    "      out = model(**inputs.to(device))\n",
    "      test_loss += calc_loss(inputs.input_ids, out.logits, attn).item() * scale_bs \n",
    "\n",
    "    test_loss /= (len(x_test)) \n",
    "  if test_loss < best_test_loss: \n",
    "    best_test_loss = test_loss \n",
    "    accelerator.wait_for_everyone() \n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(file_name, save_function=accelerator.save, state_dict=accelerator.get_state_dict(model))\n",
    "\n",
    "  model.train() \n",
    "  return best_test_loss, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 1/2: 100%|██████████| 69426/69426 [5:57:51<00:00,  3.23it/s, best_test_loss=2.75, loss=2.89, test_loss=2.75]    \n",
      "Epochs: 2/2:  78%|███████▊  | 54410/69426 [4:34:45<1:15:49,  3.30it/s, best_test_loss=2.75, loss=2.25, test_loss=2.77]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \tscheduler\u001b[39m.\u001b[39mstep() \n\u001b[1;32m     14\u001b[0m loop\u001b[39m.\u001b[39mset_description(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpochs: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mn_epochs\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m loop\u001b[39m.\u001b[39mset_postfix(loss\u001b[39m=\u001b[39mloss\u001b[39m.\u001b[39;49mitem()\u001b[39m*\u001b[39msteps, test_loss\u001b[39m=\u001b[39mtest_loss, best_test_loss\u001b[39m=\u001b[39mbest_test_loss) \n\u001b[1;32m     17\u001b[0m writer\u001b[39m.\u001b[39madd_scalar(\u001b[39m'\u001b[39m\u001b[39mcharts/learning_rate\u001b[39m\u001b[39m'\u001b[39m, optimizer\u001b[39m.\u001b[39mparam_groups[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m], global_step)\n\u001b[1;32m     18\u001b[0m writer\u001b[39m.\u001b[39madd_scalar(\u001b[39m'\u001b[39m\u001b[39mlosses/train_loss\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m.\u001b[39mitem()\u001b[39m*\u001b[39msteps, global_step)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "filename = 'lfqa_pythia_1b_'\n",
    "for epoch in range(n_epochs): \n",
    "\tfor idx, (x, attn) in (loop := tqdm(enumerate(train_dl), total=len(train_dl))): \n",
    "\t\tinputs = tokenizer(x, return_tensors='pt', max_length=MAX_LEN, padding='longest', truncation=True)\n",
    "\t\tout = model(**inputs.to(device))\n",
    "\t\tloss = calc_loss(inputs.input_ids, out.logits, attn) / steps \n",
    "\t\taccelerator.backward(loss)\n",
    "\n",
    "\t\tif idx % steps == 0: \n",
    "\t\t\toptimizer.step() \n",
    "\t\t\tmodel.zero_grad() \n",
    "\t\t\tscheduler.step() \n",
    "\n",
    "\t\tloop.set_description(f'Epochs: {epoch+1}/{n_epochs}')\n",
    "\t\tloop.set_postfix(loss=loss.item()*steps, test_loss=test_loss, best_test_loss=best_test_loss) \n",
    "\n",
    "\t\twriter.add_scalar('charts/learning_rate', optimizer.param_groups[0]['lr'], global_step)\n",
    "\t\twriter.add_scalar('losses/train_loss', loss.item()*steps, global_step)\n",
    "\n",
    "\t\tif idx % (len(train_dl) // 10) == 0: \n",
    "\t\t\tbest_test_loss, test_loss = test_it(f'models/{filename}best', best_test_loss)\n",
    "\t\t\twriter.add_scalar('losses/test_loss', test_loss, global_step)\n",
    "\n",
    "\t\tglobal_step += 1\n",
    "\n",
    "model.eval() \n",
    "best_test_loss, test_loss = test_it(f'models/{filename}best', best_test_loss)\n",
    "writer.add_scalar('losses/test_loss', test_loss, global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [f'''Context:\n",
    "* The general name of energy which has to do with location relative to something else is called potential energy. In this particular case, of course, we call it gravitational potential energy. If it is a question of electrical forces against which we are working, instead of gravitational forces, if we are “lifting” charges away from other charges with a lot of levers, then the energy content is called electrical potential energy. The general principle is that the change in the energy is the force times the distance that the force is pushed, and that this is a change in energy in general:\n",
    "* Fig. 14-3. The potential energy between two atoms as a function of the distance between them.\n",
    "* Remember that the potential φ has a physical significance: it is the potential energy which a unit charge would have if brought to the specified point in space from some reference point.\n",
    "* etc., which are acting with respect to one another in pairs due to forces all of which are conservative. In these circumstances the kinetic energy in the entire system is simply the sum of the kinetic energies of all of the particular atoms or planets or whatever, and the potential energy of the system is the sum, over the pairs of particles, of the potential energy of mutual interaction of a single pair, as though the others were not there. (This is really not true for molecular forces, and the formula is somewhat more complicated; it certainly is true for Newtonian gravitation, and it is true as an approximation for molecular forces. For molecular forces there is a potential energy, but it is sometimes a more complicated function of the positions of the atoms than simply a sum of terms from pairs.) In the special case of gravity, therefore, the potential energy is the sum, over all the pairs i and j, of Gmimj/rij, as was indicated in Eq. (). Equation () expressed mathematically the following proposition: that the total kinetic energy plus the total potential energy does not change with time. As the various planets wheel about, and turn and twist and so on, if we calculate the total kinetic energy and the total potential energy we find that the total remains constant.{tokenizer.eos_token}\n",
    "\n",
    "Question: What is Potential Energy?\n",
    "Answer:''',\n",
    "f'''Context:\n",
    "* be simple. Try to imagine what makes a drag on an airplane flying through the air—the air rushing over the wings, the swirling in the back, the changes going on around the fuselage, and many other complications, and you see that there is not going to be a simple law. On the other hand, it is a remarkable fact that the drag force on an airplane is approximately a constant times the square of the velocity, or F cv2.\n",
    "* law that can be used in the design of airplanes, but this law is not in the same class as the basic laws of physics, and further study of it will only make it more and more complicated. A study of how the coefficient c depends on the shape of the front of the airplane is, to put it mildly, frustrating. There just is no simple law for determining the coefficient in terms of the shape of the airplane. In contrast, the law of gravitation is simple, and further study only indicates its greater simplicity.\n",
    "* air—they get too heavy to be supported any longer in the updraft. As they come down, they draw a little air with them and start a downdraft. And surprisingly enough, it is easy to see that once the downdraft is started, it will maintain itself. The air now drives itself down!{tokenizer.eos_token}\n",
    "\n",
    "Question: What makes an airplane fly? \n",
    "Answer:''', \n",
    "f'''Context:\n",
    "* Where do the currents come from? One possibility would be from the motion of the electrons in atomic orbits. Actually, that is not the case for iron, although it is for some materials. In addition to moving around in an atom, an electron also spins about on its own axis—something like the spin of the earth—and it is the current from this spin that gives the magnetic field in iron. (We say “something like the spin of the earth” because the question is so deep in quantum mechanics that the classical ideas do not really describe things too well.) In most substances, some electrons spin one way and some spin the other, so the magnetism cancels out, but in iron—for a mysterious reason which we will discuss later—many of the electrons are spinning with their axes lined up, and that is the source of the magnetism.\n",
    "* In any case, we have found an induced atomic moment proportional to the magnetic field B and opposing it. This is diamagnetism of matter. It is this magnetic effect that is responsible for the small force on a piece of bismuth in a nonuniform magnetic field. (You could compute the force by working out the energy of the induced moments in the field and seeing how the energy changes as the material is moved into or out of the high-field region.)\n",
    "* We find that the induced magnetization—the magnetic moment per unit volume—is proportional to the magnetic field. This is the phenomenon of paramagnetism. You will see that the effect is stronger at lower temperatures and weaker at higher temperatures. When we put a field on a substance, it develops, for small fields, a magnetic moment proportional to the field. The ratio of M to B (for small fields) is called the magnetic susceptibility.{tokenizer.eos_token}\n",
    "\n",
    "Question: what causes magnetism? \n",
    "Answer:''', \n",
    "f'''Context:\n",
    "* Hunan First Normal University. Hunan First Normal University, founded in 1903, is a higher education institution located in Yuelu District, Changsha, Hunan Province, China.\n",
    "* Profile of Hunan First Normal College. Hunan First Normal College ---the alma mater of Mao Zedong, was founded in. 1903, and can date back to the Nan Song Dynasty when Southern Changsha City. Academy was founded. Now it is a three-year normal college, and enjoying the.\n",
    "* Hunan First Normal University covers a total area of 1346 mu, with more than 420,000 square meters of floor space. The university is divided into 10 colleges.\n",
    "* Hunan First Normal University. Public University, Changsha City, Hunan province, China. Introduction. Hunan First Normal University - the alma mater of Mao Zedong, was founded in 1903, and can date back to the Nan Song Dynasty when Southern Changsha City Academy was founded.\n",
    "* Publish your University Ranking. Established in 1903, Hunan First Normal University is a higher education institution located in the large city of Changsha (population range of 1,000,000-5,000,000 inhabitants), Hunan. Officially accredited/recognized by the Department of Education, Hunan Province, Hunan First Normal University is a coeducational higher education institution.\n",
    "* As I couldn't find the Red Hotel in the Changsha hotel section, I thought I'd review it here. It's associated with the University. Just about everything else was poor. The rooms are shabby. They provide minimal toiletries once during a three day stay.{tokenizer.eos_token}\n",
    "\n",
    "Question: How many colleges does hunan first normal university have? \n",
    "Answer:''']\n",
    "\n",
    "with torch.no_grad(): \n",
    "\tinputs = tokenizer(test, return_tensors='pt', max_length=MAX_LEN, padding='longest', truncation=True)\n",
    "\tout = model.generate(**inputs.to(device), max_length=MAX_LEN, do_sample=False, pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id)#, length_penalty=-100.0, repetition_penalty=0.01)\n",
    "\td = tokenizer.batch_decode(out)\n",
    "\tfor i in d: \n",
    "\t\tprint(i, end='\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI3_11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
